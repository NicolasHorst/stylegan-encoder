{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generate_GoT_characters_with_StyleGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate_GoT_characters_with_StyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"-QcHLdK_VGJa","colab_type":"text"},"source":["# How to Generate Game of Thrones Characters With StyleGAN\n","\n","Hi, welcome to this tutorial on generating and messsing around with Game of Thrones characters uing StyleGAN.\n","\n","If you're reading this, I'm assuming that you came here after reading the [article](https://blog.nanonets.com/stylegan-got/)  that accompanies this notebook. If you didn't, [CHECK IT OUT NOW!](https://blog.nanonets.com/stylegan-got/)\n","\n","Once you're done, we can get started with the good stuff."]},{"cell_type":"markdown","metadata":{"id":"KzXGpsu-WY97","colab_type":"text"},"source":["# Setting thigs up\n","\n","StyleGAN is a massive model that can take weeks to train. So for this tutorial, we're going to use a pretrained model that Nvidia open-sourced. You can find the official implementation [here](https://github.com/NVlabs/stylegan).\n","\n","But here, we're going to use a version of that repo that I forked and modified specifically to save you time when using Game of Thrones Characters."]},{"cell_type":"markdown","metadata":{"id":"Wd-ScVDBXQY9","colab_type":"text"},"source":["So first, let's clone the repo and move into it."]},{"cell_type":"code","metadata":{"id":"JJnVevGm4d4C","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","!pip uninstall -y tensorflow\n","!pip install tensorflow-gpu==1.14.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbhYm4y0o6F8","colab_type":"code","colab":{}},"source":["import tensorflow\n","print(tensorflow.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CGHAk5kzXUI","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","import os\n","os.chdir(\"/content/gdrive/My Drive/stylegan-encoder\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZGzyEv9LNF9","colab_type":"text"},"source":["save your face images in the raw_images folder, then execute the 2 scripts below"]},{"cell_type":"code","metadata":{"id":"thpvePFnpBL0","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/stylegan-encoder\")\n","# preprocessing step for the latent search\n","!python align_images.py raw_images/ aligned_images/\n","# the actual latent search\n","!python encode_images.py aligned_images/ generated_images/ latent_representations/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"56TsKf0lXaVd","colab_type":"text"},"source":["Next, we need to import a few required libraries. You can pretty much leave this part alone, but be sure to run it."]},{"cell_type":"code","metadata":{"id":"eJ9AzfX2IkEM","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/stylegan-encoder\")\n","import os\n","import pickle\n","import PIL.Image\n","import numpy as np\n","import dnnlib\n","import dnnlib.tflib as tflib\n","import config\n","from encoder.generator_model import Generator\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RPvHjufmXpWY","colab_type":"text"},"source":["Like I mentioned, we're going to use a pretrained StyleGAN, which was trained on the [FFHQ dataset](https://www.github.com/NVlabs/ffhq-dataset). This code block downloads the weights and initializes the network to make it easy for us to play around with the model via a high-level interface."]},{"cell_type":"code","metadata":{"id":"9wVZZNrmIkFk","colab_type":"code","colab":{}},"source":["URL_FFHQ = 'https://s3-us-west-2.amazonaws.com/nanonets/blogs/karras2019stylegan-ffhq-1024x1024.pkl'\n","\n","tflib.init_tf()\n","with dnnlib.util.open_url(URL_FFHQ, cache_dir=config.cache_dir) as f:\n","    generator_network, discriminator_network, Gs = pickle.load(f)\n","\n","generator = Generator(Gs, batch_size=1, randomize_noise=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LL_2sxIxYbl0","colab_type":"text"},"source":["We'll also define these helper functions. It may not seem intuitive now, but these functions will make it really easy to iterate quickly later on."]},{"cell_type":"code","metadata":{"id":"iGweIKvGIkGm","colab_type":"code","colab":{}},"source":["def generate_image(latent_vector):\n","    latent_vector = latent_vector.reshape((1, 18, 512))\n","    generator.set_dlatents(latent_vector)\n","    img_array = generator.generate_images()[0]\n","    img = PIL.Image.fromarray(img_array, 'RGB')\n","    #return img\n","    return img.resize((256, 256))\n","\n","def move_and_show(latent_vector, direction, coeffs):\n","    fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n","    for i, coeff in enumerate(coeffs):\n","        new_latent_vector = latent_vector.copy()\n","        new_latent_vector[:8] = (latent_vector + coeff*direction)[:8]\n","        ax[i].imshow(generate_image(new_latent_vector))\n","        ax[i].set_title('Coeff: %0.1f' % coeff)\n","    [x.axis('off') for x in ax]\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NusxTMwgYrAF","colab_type":"text"},"source":["I've also precomputed latent representations of a few popular Game of Thrones Characters. Quality isn't top-notch, though, since I did zero fine tuning on top of the baseline StyleGAN.\n","\n","[Dmitry Nikitko ](https://www.linkedin.com/in/puzer/) has an excellent [GitHub repo](https://github.com/Puzer/stylegan-encoder) which includes an \"encoder\" for StyleGAN. The repo has latent directions like smile, age, and gender built-in, so we'll load those too."]},{"cell_type":"code","metadata":{"id":"LTl1frcSIkHI","colab_type":"code","colab":{}},"source":["# Loading already learned latent directions\n","smile_direction = np.load('ffhq_dataset/latent_directions/smile.npy')\n","gender_direction = np.load('ffhq_dataset/latent_directions/gender.npy')\n","age_direction = np.load('ffhq_dataset/latent_directions/age.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cl2TURBz3PoW","colab_type":"code","colab":{}},"source":["names_list = os.listdir(\"latent_representations\")\n","latents_lfb = {n.split(\"_\")[0]: np.load(\"latent_representations/\"+n) for n in names_list if \"npy\" in n}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHxvN-Lr4qck","colab_type":"code","colab":{}},"source":["names_list = os.listdir(\"latent_representations_got\")\n","latents_got = {n.split(\".\")[0]: np.load(\"latent_representations_got/\"+n, allow_pickle=True) for n in names_list if \"npy\" in n}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gey7z2g-awnh","colab_type":"text"},"source":["If you want to play around with your own images, go ahead! All you have to do is make sure you have the right folders set up, then upload your face images to the 'raw_images' folder (without quotes) and these scripts by uncommenting the appropriate lines."]},{"cell_type":"code","metadata":{"id":"kEGubEF2eIuC","colab_type":"code","colab":{}},"source":["# First, set up the required folders\n","\n","#os.mkdir('raw_images')\n","#os.mkdir('aligned_images')\n","#os.mkdir('generated_images')\n","\n","# Then, run these scripts\n","\n","# 1) Extract and align faces from images\n","#!python align_images.py raw_images/ aligned_images/\n","\n","# 2) Find latent representation of aligned images\n","#!python encode_images.py aligned_images/ generated_images/ latent_representations/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKzLQp1QIkHd","colab_type":"text"},"source":["# Character Transformations"]},{"cell_type":"markdown","metadata":{"id":"giVvTvjTcn2S","colab_type":"text"},"source":["Using the latent representation of any character along with the learned latent directions, you can adjust three traits of any charcter: smile, age, and gender. This is done via the `move_and_show()` function, which takes three arguments: a latent vector, a latent direction, and an array of coefficients. It then performs the following computation:\n","\n","\n","$$ \\text{new latent vector} = \\text{latent vector} + (\\text{coefficient} * \\text{latent direction} )$$\n","\n","\n","The new latent vectors are passed through the StyleGAN genertor, and the resulting images are plotted. You can refer back to the previous section to go through the implementation of `move_and_show()`."]},{"cell_type":"code","metadata":{"id":"8hCWdbRqIkHe","colab_type":"code","colab":{}},"source":["move_and_show(latents_lfb[\"ChristophHaarburger\"], smile_direction, [-1, 0, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aCa6xzjOIkIF","colab_type":"code","colab":{}},"source":["move_and_show(latents_lfb[\"DennisEschweiler\"], gender_direction, [-2, 0, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeHybNDBIkIS","colab_type":"code","colab":{}},"source":["move_and_show(latents_lfb[\"JohannesStegmaier\"], age_direction, [-4, 0, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tNwW1GIPCLo","colab_type":"text"},"source":["#Character Fusion"]},{"cell_type":"markdown","metadata":{"id":"ms31vQZ6gLJ4","colab_type":"text"},"source":["We can also try mixing characters together to find an \"average\" of the two. The easiest way to do this is to simply take an average of the two latent vectors and generate an image using the average latent vector. You can also adjust how much of each character you want by changing `alpha`."]},{"cell_type":"code","metadata":{"id":"5cQRhPOfgsCY","colab_type":"code","colab":{}},"source":["alpha = 0.5\n","char1 = latents_lfb[\"LaxmiGupta\"]\n","char2 = latents_lfb[\"PhilippGraebel\"]\n","mix = (((alpha)*char1)+((1-alpha)*char2))\n","\n","move_and_show(mix, gender_direction, [-2, 0, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nyqNjr1gs3j","colab_type":"text"},"source":["Note that above, you basically created a new character, so can save that latent and do the age, gender, and smile transforms as shown above.\n","\n","The second, and perhaps more interesting way to fuse characters i through Nvidia's style mixing technique. Refer back to the [main article](https://blog.nanonets.com/stylegan-got/https://blog.nanonets.com/got-gan) to learn how this works. The code below is a modification of Nvidia's style mixing implementation."]},{"cell_type":"code","metadata":{"id":"aKPzPgxoO_LI","colab_type":"code","colab":{}},"source":["def draw_style_mixing_figure(png, Gs, w, h, src_dlatents, dst_dlatents, style_ranges):\n","    print(png)\n","    #src_dlatents = Gs.components.mapping.run(src_latents, None) # [seed, layer, component]\n","    #dst_dlatents = Gs.components.mapping.run(dst_latents, None)\n","    src_images = Gs.components.synthesis.run(src_dlatents, randomize_noise=False, **synthesis_kwargs)\n","    dst_images = Gs.components.synthesis.run(dst_dlatents, randomize_noise=False, **synthesis_kwargs)\n","\n","    canvas = PIL.Image.new('RGB', (w * (len(src_dlatents) + 1), h * (len(dst_dlatents) + 1)), 'white')\n","    for col, src_image in enumerate(list(src_images)):\n","        canvas.paste(PIL.Image.fromarray(src_image, 'RGB'), ((col + 1) * w, 0))\n","    for row, dst_image in enumerate(list(dst_images)):\n","        canvas.paste(PIL.Image.fromarray(dst_image, 'RGB'), (0, (row + 1) * h))\n","        row_dlatents = np.stack([dst_dlatents[row]] * len(src_dlatents))\n","        style_range = style_ranges[row]\n","        row_dlatents[:, style_range] = src_dlatents[:, style_range]\n","        row_images = Gs.components.synthesis.run(row_dlatents, randomize_noise=False, **synthesis_kwargs)\n","        for col, image in enumerate(list(row_images)):\n","            canvas.paste(PIL.Image.fromarray(image, 'RGB'), ((col + 1) * w, (row + 1) * h))\n","    canvas.save(png)\n","    return canvas.resize((512,512))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtZhcPoH6r5Z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQ-Lpdvc5Ta4","colab_type":"code","colab":{}},"source":["src_dlatents=np.stack(latents_got.values(), axis=0)\n","dst_dlatents=np.stack(latents_lfb.values(), axis=0)\n","\n","tflib.init_tf()\n","synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=1)\n","_Gs_cache = dict()\n","\n","draw_style_mixing_figure(os.path.join(config.result_dir, 'style-mixing_lfb.png'), Gs, w=1024, h=1024, src_dlatents=src_dlatents, dst_dlatents=dst_dlatents, style_ranges=[range(6,14)]*len(dst_dlatents))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C85yVRQd-_JG","colab_type":"code","colab":{}},"source":["src_dlatents=np.stack([latents_got[k] for k in [\"jon_2\", \"bran\", \"drogo\", \"daenerys\", \"jaime\"]], axis=0)\n","dst_dlatents=np.stack([latents_lfb[\"ChristophHaarburger\"]], axis=0)\n","\n","tflib.init_tf()\n","synthesis_kwargs = dict(output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), minibatch_size=1)\n","_Gs_cache = dict()\n","\n","draw_style_mixing_figure(os.path.join(config.result_dir, 'style-mixing_chris5.png'), Gs, w=1024, h=1024, src_dlatents=src_dlatents, dst_dlatents=dst_dlatents, style_ranges=[range(5,14)]*len(dst_dlatents))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZlTchhWTilwP","colab_type":"text"},"source":["Surprisingly, I found the simple averaging method to produce better results, and I'm not sure why that's the case. I'd encourage you to keep trying Nvidia's style mixing method with different paramets, since you might get some really cool results."]},{"cell_type":"markdown","metadata":{"id":"eoFkJGUhROyZ","colab_type":"text"},"source":["# Interpolation Videos\n"]},{"cell_type":"markdown","metadata":{"id":"cN0z_tvIjEVe","colab_type":"text"},"source":["This secion will prduce one of the most exciting reults -- GAN interpolation videos. These videos perform smooth transitions between various images, which you'll only understand if you watch one yourself. So to make your own, start by tweaking these parameters to your preferences (note: this can increase the time it takes to generate a video)."]},{"cell_type":"code","metadata":{"id":"_FhZlWX5Nks_","colab_type":"code","colab":{}},"source":["duration_sec = 5.0\n","smoothing_sec = 1.0\n","mp4_fps = 20\n","num_frames = int(np.rint(duration_sec * mp4_fps))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFpWidxaj_vx","colab_type":"text"},"source":["Next, we'll define two helper functions like the ones from the start of this notebook. Except, these will reurn image arrays instead."]},{"cell_type":"code","metadata":{"id":"FBEJdPx8J0_m","colab_type":"code","colab":{}},"source":["def generate_image_for_video(latent_vector):\n","    latent_vector = latent_vector.reshape((1, 18, 512))\n","    generator.set_dlatents(latent_vector)\n","    img_array = generator.generate_images()[0]\n","\n","    return img_array\n","  \n","  \n","def move_for_video(latent_vector, direction, coeff):\n","  \n","  new_latent_vector = latent_vector.copy()\n","  new_latent_vector[:8] = (latent_vector + coeff*direction)[:8]\n","  \n","  img_array = generate_image(new_latent_vector)\n","  \n","  return img_array"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EDyPFvRYkQLw","colab_type":"text"},"source":["Finally, run this code block to generate cool videos. The results will be saved under the reults folder. Follow the comments to generate different kinds of videos, and enjoy!"]},{"cell_type":"code","metadata":{"id":"e5KevQmsRc00","colab_type":"code","colab":{}},"source":["# If you're interpolating between two characters, set these characters here\n","\n","lfb_list = list(latents_lfb.values())\n","for i in range(len(lfb_list)-1):\n","  char1 = lfb_list[i]\n","  char2 = lfb_list[i+1]\n","\n","  # This creates an nd array that stores all the image frames fot cross-character interpolation\n","  src_images = np.stack(generate_image_for_video((0.01*alpha*char2)+((1-(0.01*alpha))*char1)) for alpha in range (100))\n","\n","\n","  # Uncomment the next line if you want to do a character transforation video, and choose the arguments as per your requirement\n","  #src_images = np.stack(move_for_video(dany_meme, smile_direction, (0.02*alpha)) for alpha in range (-100,100))\n","\n","\n","  def make_frame(t):\n","      frame_idx = int(np.clip(np.round(t * mp4_fps), 0, num_frames - 1))\n","      src_image = src_images[frame_idx]\n","      return np.array(src_image)\n","\n","  # Generate video.\n","  import moviepy.editor\n","  mp4_file = 'results/lfb_vid_{}.mp4'.format(i)\n","  mp4_codec = 'libx264'\n","  mp4_bitrate = '5M'\n","\n","  video_clip = moviepy.editor.VideoClip(make_frame, duration=duration_sec)\n","  video_clip.write_videofile(mp4_file, fps=mp4_fps, codec=mp4_codec, bitrate=mp4_bitrate)"],"execution_count":0,"outputs":[]}]}